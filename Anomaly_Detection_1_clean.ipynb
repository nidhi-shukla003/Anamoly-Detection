{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQ9nHwGoy7k"
   },
   "source": [
    "# **Anomaly Detection with ResNet for Industrial Application**: Spotlight on PatchCore and Autoencoder Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze6z_8VfoMhN"
   },
   "source": [
    "# Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rSSALsYUqO0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzOrAfwcXAtJ",
    "outputId": "57cef711-1027-4b0b-9ada-e2c671cb7f12"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download ipythonx/mvtec-ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "knZIPDQnXeFh"
   },
   "outputs": [],
   "source": [
    "!unzip -q mvtec-ad.zip -d /content/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4Goen1acUR0"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_image_path = Path('/content/data/carpet/train')\n",
    "\n",
    "good_dataset = ImageFolder(root=train_image_path, transform=transform)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(good_dataset, [0.8, 0.2])\n",
    "\n",
    "# Set the batch size\n",
    "BS = 16\n",
    "\n",
    "# Create data loaders for training and testing datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS_X-0k6efsZ"
   },
   "source": [
    "# Load pretrained Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RmTyqoAcfL2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class resnet_feature_extractor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"This class extracts the feature maps from a pretrained Resnet model.\"\"\"\n",
    "        super(resnet_feature_extractor, self).__init__()\n",
    "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Hook to extract feature maps\n",
    "        def hook(module, input, output) -> None:\n",
    "            \"\"\"This hook saves the extracted feature map on self.featured.\"\"\"\n",
    "            self.features.append(output)\n",
    "\n",
    "        self.model.layer2[-1].register_forward_hook(hook)\n",
    "        self.model.layer3[-1].register_forward_hook(hook)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        self.features = []\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input)\n",
    "\n",
    "        self.avg = torch.nn.AvgPool2d(3, stride=1)\n",
    "        fmap_size = self.features[0].shape[-2]         # Feature map sizes h, w\n",
    "        self.resize = torch.nn.AdaptiveAvgPool2d(fmap_size)\n",
    "\n",
    "        resized_maps = [self.resize(self.avg(fmap)) for fmap in self.features]\n",
    "        patch = torch.cat(resized_maps, 1)            # Merge the resized feature maps\n",
    "\n",
    "        return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "5vN0sgcZchvU",
    "outputId": "78c2e6f1-24f9-4433-ad6b-7ce0096af3ce"
   },
   "outputs": [],
   "source": [
    "image = Image.open('/content/data/carpet/test/color/000.png')\n",
    "image = transform(image).unsqueeze(0)\n",
    "\n",
    "backbone = resnet_feature_extractor()\n",
    "feature = backbone(image)\n",
    "\n",
    "print(backbone.features[0].shape)\n",
    "print(backbone.features[1].shape)\n",
    "\n",
    "print(feature.shape)\n",
    "\n",
    "plt.imshow(image[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "a8gZRnjpcsa6",
    "outputId": "b3abc302-2da5-4fff-842b-53804af4ef47"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Select 10 random indices for feature maps\n",
    "indices = torch.randperm(64)[:10]\n",
    "\n",
    "# Plot the selected feature maps\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, idx in enumerate(indices):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    axes[row, col].imshow(feature[0,idx].detach().cpu(), cmap='gray')\n",
    "    axes[row, col].set_title(f'Feature Map {idx}')\n",
    "    axes[row, col].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhSfSR_BcOzd"
   },
   "source": [
    "[link text](https://)# AutoEncoder with Resnet backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxYD9Kh0oetu"
   },
   "source": [
    "# Autoencoder Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MbvCv3Celgm"
   },
   "source": [
    "## The autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqXc4mYmcvRK"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeatCAE(nn.Module):\n",
    "    \"\"\"Autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=1000, latent_dim=50, is_bn=True):\n",
    "        super(FeatCAE, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(in_channels, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d(2 * latent_dim, latent_dim, kernel_size=1, stride=1, padding=0)]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        # if 1x1 conv to reconstruct the rgb values, we try to learn a linear combination\n",
    "        # of the features for rgb\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(latent_dim, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d(2 * latent_dim, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]\n",
    "        if is_bn:\n",
    "            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]\n",
    "        layers += [nn.ReLU()]\n",
    "        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, in_channels, kernel_size=1, stride=1, padding=0)]\n",
    "        # layers += [nn.ReLU()]\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i77A01Picxg5"
   },
   "outputs": [],
   "source": [
    "# model = FeatCAE(in_channels=1536, latent_dim=100)\n",
    "model = FeatCAE(in_channels=1536, latent_dim=100).cuda()\n",
    "backbone.cuda()\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "E1U2VAinczYG",
    "outputId": "e0df9059-34aa-4ab3-8e69-11bb7fc3b9c4"
   },
   "outputs": [],
   "source": [
    "ckpoints = torch.load('/content/autoencoder_with_resnet_deep_features.pth')\n",
    "model.load_state_dict(ckpoints)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672,
     "referenced_widgets": [
      "9f4c2b3979ff4ecc84864174dca6768c",
      "722f976f998440ba8bb4791d975b4c2c",
      "2c611d04fa4841c9b6bcf1a1bf4f61c8",
      "baef17cb0c96499a91e277055dbb77e5",
      "59b10b2be9cf47a09180967275cc8ec2",
      "bd168a718c4f4029bfa4af053d9c27ef",
      "2328f33fa1144d079104234ca9a565bb",
      "efc9eb30545249279c67425fe54c6c41",
      "7d9bb69534f041c9b526e3766fdac185",
      "39b84e73ab094d449caba0e4dcd7acbc",
      "1ef9a34955ad42bcaa86bc367b07deef"
     ]
    },
    "id": "DrVBR2FEeLj5",
    "outputId": "2bba3c3f-d5c4-4f35-a7a8-59d7354c8647"
   },
   "outputs": [],
   "source": [
    "# Define a list to store training loss and validation loss\n",
    "Loss = []\n",
    "Validation_Loss = []\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    for data,_ in train_loader:\n",
    "        with torch.no_grad():\n",
    "            features = backbone(data.cuda())\n",
    "        # Forward pass\n",
    "        output = model(features)\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, features)\n",
    "        # Backpropagation and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    Loss.append(loss.item())\n",
    "\n",
    "    # Calculate validation loss\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loss_sum = 0.0\n",
    "        num_batches = 0\n",
    "        for data, _ in test_loader:\n",
    "            features = backbone(data.cuda())\n",
    "            output = model(features)\n",
    "            val_loss = criterion(output, features)\n",
    "            val_loss_sum += val_loss.item()\n",
    "            num_batches += 1\n",
    "        val_loss_avg = val_loss_sum / num_batches\n",
    "        Validation_Loss.append(val_loss_avg)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item(), val_loss_avg))\n",
    "\n",
    "plt.plot(Loss, label='Training Loss')\n",
    "plt.plot(Validation_Loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aobql8VdeLbk"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/content/autoencoder_with_resnet_deep_features.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A77sPO8geu1Z"
   },
   "source": [
    "## Prediction of heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "XlJlL5Xoesmh",
    "outputId": "2f0964c3-07c9-4081-f4bb-d1aa7bb530c2"
   },
   "outputs": [],
   "source": [
    "image = Image.open('/content/data/carpet/test/color/000.png')\n",
    "image = transform(image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = backbone(image.cuda())\n",
    "    # features = backbone(image)\n",
    "    recon = model(features)\n",
    "\n",
    "recon_error =  ((features-recon)**2).mean(axis=(1)).unsqueeze(0)\n",
    "\n",
    "segm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution\n",
    "                recon_error,\n",
    "                size=(224, 224),\n",
    "                mode='bilinear'\n",
    "            )\n",
    "\n",
    "plt.imshow(segm_map.squeeze().cpu().numpy(), cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5YBLc8Ye9qH"
   },
   "source": [
    "## For OK images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjgSIDhzfBTy"
   },
   "outputs": [],
   "source": [
    "def decision_function(segm_map):\n",
    "\n",
    "    mean_top_10_values = []\n",
    "\n",
    "    for map in segm_map:\n",
    "        # Flatten the tensor\n",
    "        flattened_tensor = map.reshape(-1)\n",
    "\n",
    "        # Sort the flattened tensor along the feature dimension (descending order)\n",
    "        sorted_tensor, _ = torch.sort(flattened_tensor,descending=True)\n",
    "\n",
    "        # Take the top 10 values along the feature dimension\n",
    "        mean_top_10_value = sorted_tensor[:10].mean()\n",
    "\n",
    "        mean_top_10_values.append(mean_top_10_value)\n",
    "\n",
    "    return torch.stack(mean_top_10_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHygYEsbfO7U"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "RECON_ERROR=[]\n",
    "for data,_ in train_loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = backbone(data.cuda()).squeeze()\n",
    "        # features = backbone(data).squeeze()\n",
    "        # Forward pass\n",
    "        recon = model(features)\n",
    "    # Compute the loss\n",
    "    segm_map =  ((features-recon)**2).mean(axis=(1))[:,3:-3,3:-3]\n",
    "    anomaly_score = decision_function(segm_map)\n",
    "    # anomaly_score = segm_map.mean(axis=(1,2))\n",
    "\n",
    "    RECON_ERROR.append(anomaly_score)\n",
    "\n",
    "RECON_ERROR = torch.cat(RECON_ERROR).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "JJ3m_oCufRFz",
    "outputId": "65b22626-e806-471b-a4f7-b078e81cd51c"
   },
   "outputs": [],
   "source": [
    "best_threshold = np.mean(RECON_ERROR) + 3 * np.std(RECON_ERROR)\n",
    "\n",
    "heat_map_max, heat_map_min = np.max(RECON_ERROR), np.min(RECON_ERROR)\n",
    "\n",
    "plt.hist(RECON_ERROR,bins=50)\n",
    "plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MfMEr6OfUgF"
   },
   "source": [
    "## For Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcPb5fq3fWEC"
   },
   "outputs": [],
   "source": [
    "y_true=[]\n",
    "y_pred=[]\n",
    "y_score=[]\n",
    "\n",
    "model.eval()\n",
    "backbone.eval()\n",
    "\n",
    "test_path = Path('/content/data/carpet/test')\n",
    "\n",
    "for path in test_path.glob('*/*.png'):\n",
    "    fault_type = path.parts[-2]\n",
    "    test_image = transform(Image.open(path)).cuda().unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = backbone(test_image)\n",
    "        # Forward pass\n",
    "        recon = model(features)\n",
    "\n",
    "    segm_map = ((features - recon)**2).mean(axis=(1))[:,3:-3,3:-3]\n",
    "    y_score_image = decision_function(segm_map=segm_map)\n",
    "    # y_score_image = segm_map.mean(axis=(1,2))\n",
    "\n",
    "    y_pred_image = 1*(y_score_image >= best_threshold)\n",
    "\n",
    "    y_true_image = 0 if fault_type == 'good' else 1\n",
    "\n",
    "    y_true.append(y_true_image)\n",
    "    y_pred.append(y_pred_image.cpu().numpy())\n",
    "    y_score.append(y_score_image.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_score = np.array(y_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "856KudgKfWXz",
    "outputId": "9df73fbb-ece0-4df5-bddb-a3558a2cf404"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_score,bins=50)\n",
    "plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "id": "Y9aWzjGbfeBt",
    "outputId": "4b09ba3f-0a62-4c6e-d03d-f55f26257dd6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc_roc_score = roc_auc_score(y_true, y_score)\n",
    "print(\"AUC-ROC Score:\", auc_roc_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "f1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]\n",
    "\n",
    "# Select the best threshold based on F1 score\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f'best_threshold = {best_threshold}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, (y_score >= best_threshold).astype(int))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['OK','NOK'])\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPh1YXZufiNA"
   },
   "source": [
    "## Printout the prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osqr8G0wfkAN",
    "outputId": "8fbaf771-374e-4a16-c3f2-9925b4bae782"
   },
   "outputs": [],
   "source": [
    "import cv2, time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model.eval()\n",
    "backbone.eval()\n",
    "\n",
    "test_path = Path('/content/data/carpet/test')\n",
    "\n",
    "for path in test_path.glob('*/*.png'):\n",
    "    fault_type = path.parts[-2]\n",
    "    test_image = transform(Image.open(path)).cuda().unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = backbone(test_image)\n",
    "        # Forward pass\n",
    "        recon = model(features)\n",
    "\n",
    "    segm_map = ((features - recon)**2).mean(axis=(1))\n",
    "    y_score_image = decision_function(segm_map=segm_map)\n",
    "    # y_score_image = segm_map.mean(axis=(1,2))\n",
    "\n",
    "    y_pred_image = 1*(y_score_image >= best_threshold)\n",
    "    class_label = ['OK','NOK']\n",
    "\n",
    "    if fault_type in ['thread']:\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(test_image.squeeze().permute(1,2,0).cpu().numpy())\n",
    "        plt.title(f'fault type: {fault_type}')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        heat_map = segm_map.squeeze().cpu().numpy()\n",
    "        heat_map = heat_map\n",
    "        heat_map = cv2.resize(heat_map, (128,128))\n",
    "        plt.imshow(heat_map, cmap='jet', vmin=heat_map_min, vmax=heat_map_max*10) # Here I am cheating by multiplying by 10 (obtained using trail error)\n",
    "        plt.title(f'Anomaly score: {y_score_image[0].cpu().numpy() / best_threshold:0.4f} || {class_label[y_pred_image]}')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow((heat_map > best_threshold * 10), cmap='gray')\n",
    "        plt.title(f'segmentation map')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # time.sleep(0.05)\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "        # break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kbIzYS9f2f0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4hfFsHbcHCD"
   },
   "source": [
    "# PatchCore Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsaQJKAYUqO5"
   },
   "source": [
    "## Create memory bank from 'normal' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "922d468425df40148ff96492605bd0ba",
      "7a130160e861405d8e44aff3d643e1ef",
      "a27a2f330f96435fb7ef7fcfd8d5235e",
      "c8d50b6a6b824e3390bfeb67fdf855e3",
      "4366b42adca541849828f15322a7f92d",
      "2392874dfad343dda1e9921af6a76b0d",
      "8074f9b6ec23420b90c64dab9ffde881",
      "aa2f3b72c5304490bde9230553b568a9",
      "0a1e679c74a54965b03199cf31d2b5ae",
      "775edb18c649494c80e29cb21ee37669",
      "d07088be5e974811bb32a95abb57e8ef"
     ]
    },
    "id": "U8cJH9OgUqO5",
    "outputId": "3d575d6d-af8d-4999-fc31-46233fd6347b"
   },
   "outputs": [],
   "source": [
    "memory_bank =[]\n",
    "\n",
    "folder_path = Path('/content/data/carpet/train/good')\n",
    "\n",
    "for pth in tqdm(folder_path.iterdir(),leave=False):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data = transform(Image.open(pth)).cuda().unsqueeze(0)\n",
    "        features = backbone(data)\n",
    "        features = features.reshape(features.shape[1], -1).T\n",
    "        memory_bank.append(features.cpu().detach())\n",
    "\n",
    "memory_bank = torch.cat(memory_bank,dim=0).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqPKHTY9UqO5"
   },
   "source": [
    "## Only select 10% of total patches to avoid long inference time and computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzEXCSNkUqO5"
   },
   "outputs": [],
   "source": [
    "selected_indices = np.random.choice(len(memory_bank), size=len(memory_bank)//10, replace=False)\n",
    "memory_bank = memory_bank[selected_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n20pJHLhUqO5"
   },
   "source": [
    "## For OK images [K nearsest neighbours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "cbd9fc3037d24e74829e628647da0504",
      "e7b6ae8a18364b6ca1043cdc93cbd7aa",
      "00acc837ee7d4ba985afe5c491a56efd",
      "f3ef05d33f33409a8349779d8c055761",
      "68667b832bb948659b4a5feb10fcf865",
      "9f07b932d0524c42abf7dccd53498b58",
      "43f7de3b776c4323bd6ba3fe4b66ae09",
      "a488d588c9064c3f843b59344b925ab8",
      "4410ae3bbd924f998217a58e965db38c",
      "92f40a647ba343c1b50dc17b2f41c6be",
      "3048080cefaf41eba5a1cd4ec243f204"
     ]
    },
    "id": "9aAsCjUVUqO5",
    "outputId": "66514cad-fd90-4a46-fc87-6d5f432895ec"
   },
   "outputs": [],
   "source": [
    "y_score=[]\n",
    "folder_path = Path('/content/data/carpet/train/good')\n",
    "\n",
    "for pth in tqdm(folder_path.iterdir(),leave=False):\n",
    "    data = transform(Image.open(pth)).cuda().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = backbone(data)\n",
    "        features = features.reshape(features.shape[1], -1).T\n",
    "    distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "    dist_score, dist_score_idxs = torch.min(distances, dim=1)\n",
    "    s_star = torch.max(dist_score)\n",
    "    segm_map = dist_score.view(1, 1, 28, 28)\n",
    "\n",
    "    y_score.append(s_star.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "GpqYH2gTUqO6",
    "outputId": "cdb04443-be0d-4c84-d2ab-9de77f243b52"
   },
   "outputs": [],
   "source": [
    "best_threshold = np.mean(y_score) + 2 * np.std(y_score)\n",
    "\n",
    "plt.hist(y_score,bins=50)\n",
    "plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2keL7VbUqO6"
   },
   "source": [
    "## For NOK Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "1a6dcee336e443a6aa375aa47fcc1dc5",
      "05b5a4c727f34891be1542113ad9d807",
      "ba6f751e510b466b86fbd49f80bbf138",
      "9548271cbb8544e4803b63819253a1c5",
      "d5de386eb34e454d922b92f30ffd2461",
      "a9d427dbdcdc44c7806c3596b758ae8b",
      "c485034464704cc0bb91143a27460307",
      "82bf356257884c11bd5f802628301144",
      "5087f1ceafff449997eaf0de03a1a142",
      "e3cb1254dd3f47d6a0beef79314d0b03",
      "c1ca1b66208947e79fa5eee2b8dc41f4",
      "87bdbc091c254a83b08d29fa8e8a8e42",
      "6f0bfa1c20d04d5088e535f36c1932c6",
      "1c80dadfc7c648518897036dfdecf785",
      "b2ccc55b901a4855bfc2ba7e5f0da4ee",
      "56d4390d7c81417f9f667b9399375c74",
      "cb8d6d6477e84ee1ae2f456b921488a6",
      "e42bd8fa575549dc84eb189bf345623e",
      "18865c1c40fc4d21b14fcf20d9f8f14b",
      "6179f22a40ee41b49c21c69b0cf32e76",
      "4e07f41b587f49188f17e3d9204b923e",
      "5dac210e71da4606a7a8ef91f865c09b",
      "cc7f0a3aacee4ee0956882849cf23a03",
      "36c5a47977d049eab50517ed5e739e58",
      "f97e9355fb0e4f11bfc3ea88387f09be",
      "e12d47e81e964e1e9396550aaa7c7044",
      "71125fef90934e28ac1c798afad66c02",
      "7ff4f8c32ad64851be0952065e7db439",
      "1e5ab99e95d341e8b311c282d233fd16",
      "7e26f56c42da4054b8341ee870e8aff2",
      "caf1694c6c0c4734a93c00bc5786be16",
      "1c62bcb9aca74ff68630efa5b9314ead",
      "17128831c3da4b39a59f1d18377fc727",
      "f55c3acea749414a8c781c4604c3642d",
      "2e9a152a64e74ff18e3b929d9ac44f7a",
      "b321dc8e33604f0dbcceb6ab91ac50fe",
      "c35c899f796944ff8cb03b43f2715795",
      "ceb0ad29b64f48fbab73be416c8985ea",
      "47995073453b4f04b19df6e2481af443",
      "4e0209230dde43de93b8bf5ebcc96f27",
      "8e37b8253b4f42449d080c63394b7767",
      "ba86900370714504b5c9df21e0b3a093",
      "5e07b96a8f4746b180faf355b189af68",
      "7e54a74e28a249c8a87fa184d4876c05",
      "f845bdd1857443b697642fc1e8ee4486",
      "9c0338a8b5244a97802e8bfcee7c1994",
      "0dae507d5f234f5092ea4ce7eeb1007a",
      "de9fae625ed542548df94a847e22a9d9",
      "7d156cce86ef4d84843d6cd2341514af",
      "0932a5bbf9f04195811d05f768d0dd8a",
      "654802501d1c4cebaae352a23e225ee0",
      "303e285c944d463fb747c617b21053d3",
      "3afe7c0f3d934957be0c2ffe0fcb9e25",
      "4b151bcb602541b095a825356453a747",
      "0dcd92f31e9244418db8c2645c151349",
      "de2a93766fb64c1e8f782db924114e52",
      "d244151072d3470f823cbc2e749ddd85",
      "eb30781b53f349d6a36b897517f8ec95",
      "92c5eace52074424ba586e02d9d2f338",
      "fa1c537c2213441ba9727ce366df72ff",
      "95cb9a239eeb4eb6971cec3233229c6b",
      "9a2ca24658354b6a8e21a461f18adf50",
      "3068f241ccd44aca833514087a8baa08",
      "f69eb0d69d414f9c8feb327a489df044",
      "70550e6b0130482bbee3e49eb441d9cb",
      "9a11986cf8a642f49d44f387399fc548"
     ]
    },
    "id": "4-Qm63_qUqO6",
    "outputId": "21bebfd2-5015-46e1-9e16-0001799929a0"
   },
   "outputs": [],
   "source": [
    "y_score = []\n",
    "y_true=[]\n",
    "\n",
    "for classes in ['color','good','cut','hole','metal_contamination','thread']:\n",
    "    folder_path = Path('/content/data/carpet/test/{}'.format(classes))\n",
    "\n",
    "    for pth in tqdm(folder_path.iterdir(),leave=False):\n",
    "\n",
    "        class_label = pth.parts[-2]\n",
    "        with torch.no_grad():\n",
    "            test_image = transform(Image.open(pth)).cuda().unsqueeze(0)\n",
    "            features = backbone(test_image)\n",
    "            features = features.reshape(features.shape[1], -1).T\n",
    "        distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "        dist_score, dist_score_idxs = torch.min(distances, dim=1)\n",
    "        s_star = torch.max(dist_score)\n",
    "        segm_map = dist_score.view(1, 1, 28, 28)\n",
    "\n",
    "        y_score.append(s_star.cpu().numpy())\n",
    "        y_true.append(0 if class_label == 'good' else 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "OPrTlSudUqO6",
    "outputId": "12640d18-f6c1-4e49-d363-cb01022864a4"
   },
   "outputs": [],
   "source": [
    "# plotting the y_score values which do not belong to 'good' class\n",
    "\n",
    "y_score_nok = [score  for score,true in zip(y_score,y_true) if true==1]\n",
    "plt.hist(y_score_nok,bins=50)\n",
    "plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "GQEzqWM1UqO6",
    "outputId": "6aa7fa8b-39b4-43e9-958c-fd72d23ec1a7"
   },
   "outputs": [],
   "source": [
    "test_image = transform(Image.open('/content/data/carpet/test/color/000.png')).cuda().unsqueeze(0)\n",
    "features = backbone(test_image)\n",
    "features = features.reshape(features.shape[1], -1).T\n",
    "distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "dist_score, dist_score_idxs = torch.min(distances, dim=1)\n",
    "s_star = torch.max(dist_score)\n",
    "segm_map = dist_score.view(1, 1, 28, 28)\n",
    "\n",
    "segm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution\n",
    "                segm_map,\n",
    "                size=(224, 224),\n",
    "                mode='bilinear'\n",
    "            )\n",
    "\n",
    "plt.imshow(segm_map.cpu().squeeze(), cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1SY-zk3UqO7"
   },
   "source": [
    "## Evaluation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "id": "mvvpG_Y3UqO7",
    "outputId": "f6001f53-c2d7-4b89-8275-aa073d79a22e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc_roc_score = roc_auc_score(y_true, y_score)\n",
    "print(\"AUC-ROC Score:\", auc_roc_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "f1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]\n",
    "\n",
    "# Select the best threshold based on F1 score\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f'best_threshold = {best_threshold}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, (y_score >= best_threshold).astype(int))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['OK','NOK'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k59dMrD1UqO7"
   },
   "source": [
    "## Printout the prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5vxwzj5nUqO7",
    "outputId": "109e2a91-bd66-49e6-ba15-5b94f49e287e"
   },
   "outputs": [],
   "source": [
    "import cv2, time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "backbone.eval()\n",
    "\n",
    "test_path = Path('/content/data/carpet/test')\n",
    "\n",
    "for path in test_path.glob('*/*.png'):\n",
    "\n",
    "    fault_type = path.parts[-2]\n",
    "\n",
    "    if fault_type in ['cut']:\n",
    "\n",
    "        test_image = transform(Image.open(path)).cuda().unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = backbone(test_image)\n",
    "            features = features.reshape(features.shape[1], -1).T\n",
    "        # Forward pass\n",
    "        distances = torch.cdist(features, memory_bank, p=2.0)\n",
    "        dist_score, dist_score_idxs = torch.min(distances, dim=1)\n",
    "        s_star = torch.max(dist_score)\n",
    "        segm_map = dist_score.view(1, 1, 28, 28)\n",
    "        segm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution\n",
    "                    segm_map,\n",
    "                    size=(224, 224),\n",
    "                    mode='bilinear'\n",
    "                ).cpu().squeeze().numpy()\n",
    "\n",
    "        y_score_image = s_star.cpu().numpy()\n",
    "\n",
    "        y_pred_image = 1*(y_score_image >= best_threshold)\n",
    "        class_label = ['OK','NOK']\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(test_image.squeeze().permute(1,2,0).cpu().numpy())\n",
    "        plt.title(f'fault type: {fault_type}')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        heat_map = segm_map\n",
    "        plt.imshow(heat_map, cmap='jet',vmin=best_threshold, vmax=best_threshold*2)\n",
    "        plt.title(f'Anomaly score: {y_score_image / best_threshold:0.4f} || {class_label[y_pred_image]}')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow((heat_map > best_threshold*1.25), cmap='gray')\n",
    "        plt.title(f'segmentation map')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # time.sleep(0.05)\n",
    "        # clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "018_jtxQi7n1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
